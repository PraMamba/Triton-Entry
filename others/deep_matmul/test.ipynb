{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from typing import Tuple\n",
    "import triton\n",
    "\n",
    "import deep_gemm\n",
    "from deep_gemm import bench_kineto, calc_diff, cell_div, get_col_major_tma_aligned_tensor\n",
    "import transformer_engine.pytorch as te\n",
    "from transformer_engine.common import recipe\n",
    "import pandas as pd\n",
    "from core import per_token_cast_to_fp8, per_block_cast_to_fp8, deep_matmul, DeepLinear\n",
    "import os\n",
    "os.environ['TRITON_PRINT_AUTOTUNING'] = '1'\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_token_cast_to_fp82(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert x.dim() == 2 and x.size(1) % 128 == 0\n",
    "    m, n = x.shape\n",
    "    x_view = x.view(m, -1, 128)\n",
    "    x_amax = x_view.abs().float().amax(dim=2).view(m, -1).clamp(1e-4)\n",
    "    return (x_view * (448.0 / x_amax.unsqueeze(2))).to(torch.float8_e4m3fn).view(m, n), (x_amax / 448.0).view(m, -1)\n",
    "\n",
    "\n",
    "def per_block_cast_to_fp82(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert x.dim() == 2\n",
    "    m, n = x.shape\n",
    "    x_padded = torch.zeros((cell_div(m, 128) * 128, cell_div(n, 128) * 128), dtype=x.dtype, device=x.device)\n",
    "    x_padded[:m, :n] = x\n",
    "    x_view = x_padded.view(-1, 128, x_padded.size(1) // 128, 128)\n",
    "    x_amax = x_view.abs().float().amax(dim=(1, 3), keepdim=True).clamp(1e-4)\n",
    "    x_scaled = (x_view * (448.0 / x_amax)).to(torch.float8_e4m3fn)\n",
    "    return x_scaled.view_as(x_padded)[:m, :n].contiguous(), (x_amax / 448.0).view(x_view.size(0), x_view.size(2))\n",
    "\n",
    "\n",
    "def construct(x, y):\n",
    "    m = x.size(0)\n",
    "    n = y.size(0)\n",
    "    out = torch.empty((m, n), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    x_fp8, y_fp8 = per_token_cast_to_fp82(x), per_block_cast_to_fp82(y)\n",
    "    # Transpose earlier so that the testing will not trigger transposing kernels\n",
    "    x_fp8 = (x_fp8[0], get_col_major_tma_aligned_tensor(x_fp8[1]))\n",
    "    return x_fp8, y_fp8, out\n",
    "\n",
    "# def construct_grouped(list_x, list_y):\n",
    "#     list_x_fp8 = []\n",
    "#     list_y_fp8 = []\n",
    "#     list_out = []\n",
    "#     for idx in range(len(list_x)):\n",
    "\n",
    "def construct_grouped(x, y, is_masked=False) -> \\\n",
    "        Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor]:\n",
    "    num_groups, m,k = x.shape\n",
    "    n = y.size(1)\n",
    "    out = torch.empty((num_groups, m, n), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    assert m % 4 == 0, f'TMA alignment error: {m}'\n",
    "    x_fp8 = (torch.empty_like(x, dtype=torch.float8_e4m3fn), torch.empty((num_groups, m, k // 128), device='cuda', dtype=torch.float))\n",
    "    y_fp8 = (torch.empty_like(y, dtype=torch.float8_e4m3fn), torch.empty((num_groups, (n + 127) // 128, k // 128), device='cuda', dtype=torch.float))\n",
    "    for i in range(num_groups):\n",
    "        x_fp8[0][i], x_fp8[1][i] = per_token_cast_to_fp8(x[i])\n",
    "        y_fp8[0][i], y_fp8[1][i] = per_block_cast_to_fp8(y[i])\n",
    "\n",
    "    # For non-masked input, we must merge the group and M dims\n",
    "    if not is_masked:\n",
    "        x_fp8 = (x_fp8[0].view(-1, k), per_token_cast_to_fp8(x.view(-1, k))[1])\n",
    "        out = out.view(-1, n)\n",
    "\n",
    "    # Transpose earlier so that the testing will not trigger transposing kernels\n",
    "    x_fp8 = (x_fp8[0], get_col_major_tma_aligned_tensor(x_fp8[1]))\n",
    "    return x_fp8, y_fp8, out\n",
    "\n",
    "\n",
    "\n",
    "def deep_matmul2(x, y):\n",
    "    x_fp8, y_fp8, out = construct(x, y)\n",
    "    deep_gemm.gemm_fp8_fp8_bf16_nt(x_fp8, y_fp8, out)\n",
    "    return out\n",
    "\n",
    "def group_deep_matmul(x, y):\n",
    "    num_groups, m, k = x.shape\n",
    "    x_fp8, y_fp8, out = construct_grouped(x, y)\n",
    "    m_indices = torch.arange(0, num_groups, device='cuda', dtype=torch.int)\n",
    "    m_indices = m_indices.unsqueeze(-1).expand(num_groups, m).contiguous().view(-1)\n",
    "    deep_gemm.m_grouped_gemm_fp8_fp8_bf16_nt_contiguous(x_fp8, y_fp8, out, m_indices)\n",
    "    return out\n",
    "\n",
    "fp8_recipe = recipe.DelayedScaling(margin=0, fp8_format=recipe.Format.E4M3)\n",
    "def te_matmul(x, fc):\n",
    "    with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "        out = fc(x)\n",
    "    return out\n",
    "\n",
    "def group_te_matmul(x, fc, num_groups):\n",
    "    splits = [x.size(0)//num_groups] * num_groups\n",
    "    with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "        out = fc(x, splits)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.bfloat16\n",
    "m, n, k = 4096, 4096*2, 4096*2\n",
    "x = torch.randn(m, n, dtype=dtype, device=device)\n",
    "fc = torch.nn.Linear(k, n, bias=False, device=device, dtype=dtype)\n",
    "y = fc.weight\n",
    "x_fp8, y_fp8, out = construct(x, y)\n",
    "out_ref = fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fp8 = per_token_cast_to_fp8(x)\n",
    "b_fp8 = per_block_cast_to_fp8(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = deep_matmul2(x, y)\n",
    "out2 = deep_matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1133, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>) tensor(0.0167, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>) tensor(0.0166, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((out1 - out_ref).abs().max(), (out1 - out_ref).abs().mean())\n",
    "print((out2 - out_ref).abs().max(), (out2 - out_ref).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8789821863174438\n",
      "0.5845611095428467\n",
      "0.7825400233268738\n"
     ]
    }
   ],
   "source": [
    "print(triton.testing.do_bench(lambda:deep_matmul2(x, y)))\n",
    "print(triton.testing.do_bench(lambda:deep_matmul(x, y)))\n",
    "print(triton.testing.do_bench(lambda:fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.bfloat16\n",
    "m, n, k = 4096*2, 4096*4, 4096\n",
    "x1 = torch.randn(m, k, dtype=dtype, device=device)\n",
    "x1.requires_grad_(True)\n",
    "x2 = deepcopy(x1)\n",
    "bias = False\n",
    "fc1 = torch.nn.Linear(k, n, bias=bias, device=device, dtype=dtype)\n",
    "fc2 = DeepLinear(k, n, bias=bias, device=device, dtype=dtype)\n",
    "fc2.weight.data.copy_(fc1.weight.data)\n",
    "if bias:\n",
    "    fc2.bias.data.copy_(fc1.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1328, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>) tensor(0.0167, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1406, device='cuda:0', dtype=torch.bfloat16) tensor(0.0190, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(11., device='cuda:0', dtype=torch.bfloat16) tensor(1.5312, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "y1 = fc1(x1)\n",
    "y2 = fc2(x2)\n",
    "dy = torch.rand_like(y1)\n",
    "y1.backward(dy)\n",
    "y2.backward(dy)\n",
    "print((y1 - y2).abs().max(), (y1 - y2).abs().mean())\n",
    "print((x1.grad - x2.grad).abs().max(), (x1.grad - x2.grad).abs().mean())\n",
    "print((fc1.weight.grad - fc2.weight.grad).abs().max(), (fc1.weight.grad - fc2.weight.grad).abs().mean())\n",
    "if bias:\n",
    "    print((fc1.bias.grad - fc2.bias.grad).abs().max(), (fc1.bias.grad - fc2.bias.grad).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5591166019439697\n",
      "1.143518328666687\n",
      "3.2884576320648193\n",
      "2.541518211364746\n"
     ]
    }
   ],
   "source": [
    "print(triton.testing.do_bench(lambda:fc1(x1)))\n",
    "print(triton.testing.do_bench(lambda:fc2(x2)))\n",
    "y1 = fc1(x1)\n",
    "y2 = fc2(x2)\n",
    "dy = torch.rand_like(y1)\n",
    "print(triton.testing.do_bench(lambda:y1.backward(dy, retain_graph=True), grad_to_none=[x1, fc1.weight]))\n",
    "print(triton.testing.do_bench(lambda:y2.backward(dy, retain_graph=True), grad_to_none=[x2, fc2.weight]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_engine.pytorch.module.deep_linear import Linear as TeLinear\n",
    "from transformer_engine.pytorch.module.deep_layernorm_linear import LayerNormLinear as TeLayerNormLinear\n",
    "\n",
    "import transformer_engine.pytorch as te\n",
    "from transformer_engine.common import recipe\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp8_recipe = recipe.DelayedScaling(margin=0, fp8_format=recipe.Format.E4M3)\n",
    "# with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "device = 'cuda'\n",
    "dtype = torch.bfloat16\n",
    "m, n, k = 256, 512, 1024\n",
    "x1 = torch.randn(m, k, dtype=dtype, device=device)\n",
    "x1.requires_grad_(True)\n",
    "x2 = deepcopy(x1)\n",
    "bias = False\n",
    "fc1 = TeLinear(k, n, bias=bias, device=device, params_dtype=dtype)\n",
    "fc2 = TeLinear(k, n, bias=bias, device=device, params_dtype=dtype)\n",
    "# fc2 = te.Linear(k, n, bias=bias, device=device, params_dtype=dtype)\n",
    "fc2.weight.data.copy_(fc1.weight.data)\n",
    "if bias:\n",
    "    fc2.bias.data.copy_(fc1.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1328, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>) tensor(0.0217, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0508, device='cuda:0', dtype=torch.bfloat16) tensor(0.0087, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(1.6406, device='cuda:0', dtype=torch.bfloat16) tensor(0.2637, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "y1 = fc1(x1)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    y2 = fc2(x2)\n",
    "dy = torch.rand_like(y1)\n",
    "y1.backward(dy)\n",
    "y2.backward(dy)\n",
    "print((y1 - y2).abs().max(), (y1 - y2).abs().mean())\n",
    "print((x1.grad - x2.grad).abs().max(), (x1.grad - x2.grad).abs().mean())\n",
    "print((fc1.weight.grad - fc2.weight.grad).abs().max(), (fc1.weight.grad - fc2.weight.grad).abs().mean())\n",
    "if bias:\n",
    "    print((fc1.bias.grad - fc2.bias.grad).abs().max(), (fc1.bias.grad - fc2.bias.grad).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp8_recipe = recipe.DelayedScaling(margin=0, fp8_format=recipe.Format.E4M3)\n",
    "device = 'cuda'\n",
    "dtype = torch.bfloat16\n",
    "m, n, k = 256, 512, 1024\n",
    "x1 = torch.randn(m, k, dtype=dtype, device=device)\n",
    "x1.requires_grad_(True)\n",
    "x2 = deepcopy(x1)\n",
    "bias = False\n",
    "fc1 = TeLayerNormLinear(k, n, normalization='RMSNorm', bias=bias, device=device, params_dtype=dtype)\n",
    "fc2 = TeLayerNormLinear(k, n, normalization='RMSNorm', bias=bias, device=device, params_dtype=dtype)\n",
    "# fc2 = te.Linear(k, n, bias=bias, device=device, params_dtype=dtype)\n",
    "fc2.weight.data.copy_(fc1.weight.data)\n",
    "fc2.layer_norm_weight.data.copy_(fc1.layer_norm_weight.data)\n",
    "if bias:\n",
    "    fc2.bias.data.copy_(fc1.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>) tensor(0.0215, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', dtype=torch.bfloat16) tensor(0.0088, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(1.6562, device='cuda:0', dtype=torch.bfloat16) tensor(0.2676, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.8750, device='cuda:0', dtype=torch.bfloat16) tensor(0.1328, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "y1 = fc1(x1)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    y2 = fc2(x2)\n",
    "dy = torch.rand_like(y1)\n",
    "y1.backward(dy)\n",
    "y2.backward(dy)\n",
    "print((y1 - y2).abs().max(), (y1 - y2).abs().mean())\n",
    "print((x1.grad - x2.grad).abs().max(), (x1.grad - x2.grad).abs().mean())\n",
    "print((fc1.weight.grad - fc2.weight.grad).abs().max(), (fc1.weight.grad - fc2.weight.grad).abs().mean())\n",
    "print((fc1.layer_norm_weight.grad - fc2.layer_norm_weight.grad).abs().max(), (fc1.layer_norm_weight.grad - fc2.layer_norm_weight.grad).abs().mean())\n",
    "if bias:\n",
    "    print((fc1.bias.grad - fc2.bias.grad).abs().max(), (fc1.bias.grad - fc2.bias.grad).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
