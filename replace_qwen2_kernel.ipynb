{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/mdy/miniforge/envs/mdy/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/mnt/workspace/mdy/miniforge/envs/mdy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import importlib\n",
    "from core.fused_add_norm import triton_fused_add_norm\n",
    "from core.fused_silu import triton_fused_up_gate_silu\n",
    "from core.rmsnorm import triton_rmsnorm\n",
    "module = importlib.import_module('transformers.models.qwen2.modeling_qwen2')\n",
    "from transformers import AutoConfig\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import *\n",
    "config = AutoConfig.from_pretrained('/mnt/workspace/mdy/models/Qwen2.5-7B-Instruct')\n",
    "dtype = torch.bfloat16\n",
    "old = Qwen2DecoderLayer(config, layer_idx=0).cuda().to(dtype)\n",
    "x = torch.randn(32, 512, config.hidden_size).cuda().to(dtype)\n",
    "cos = torch.randn(32, 512, config.hidden_size//config.num_attention_heads).cuda().to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.977498292922974"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "for i in range(100):\n",
    "    old_y = old(x, position_embeddings=(cos,cos))\n",
    "t1 = time.time() - t\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm_forward(self, hidden_state):\n",
    "    return triton_rmsnorm(hidden_state, self.weight, self.variance_epsilon)\n",
    "\n",
    "def mlp_forward(self, hidden_state):\n",
    "    return self.down_proj(triton_fused_up_gate_silu(self.up_proj(hidden_state),\n",
    "                                                    self.gate_proj(hidden_state)))\n",
    "\n",
    "def decoder_layer_forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions = False,\n",
    "        use_cache = False,\n",
    "        cache_position = None,\n",
    "        position_embeddings = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, sequence_length)` where padding elements are indicated by 0.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
    "                Indices depicting the position of the input sequence tokens in the sequence.\n",
    "            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n",
    "                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n",
    "                with `head_dim` being the embedding dimension of each attention head.\n",
    "            kwargs (`dict`, *optional*):\n",
    "                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n",
    "                into the model\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "\n",
    "        hidden_states, residual = triton_fused_add_norm(hidden_states, \n",
    "                                                        residual, \n",
    "                                                        self.post_attention_layernorm.weight, \n",
    "                                                        self.post_attention_layernorm.variance_epsilon)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# module.Qwen2RMSNorm.forward = rmsnorm_forward\n",
    "# module.Qwen2MLP.forward = mlp_forward\n",
    "# module.Qwen2DecoderLayer = decoder_layer_forward\n",
    "from replace_qwen2_kernel import trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "for i in range(5000):\n",
    "    old_y = old(x, position_embeddings=(cos,cos))\n",
    "t2 = time.time() - t\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
